{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd \"/content/gdrive/MyDrive/Colab Notebooks\"\n","!ls\n","%cd \"__CS5260/assignment6/ViT with ColossalAI\"\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZxP88UBxxpm","executionInfo":{"status":"ok","timestamp":1712396455961,"user_tz":-480,"elapsed":21852,"user":{"displayName":"Weng Soon Cheah","userId":"01948259457530973069"}},"outputId":"c8a4698c-fad7-4451-e679-7d855e471389"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks\n"," assignmen3_1.ipynb\t      main-1029_1430.ipynb    UniqueClassCount-main-soon5.zip\n","'Copy of ViT_Cifar10.ipynb'   main.ipynb\t     'Untitled0 (1).ipynb'\n"," __CS5260\t\t      main_resnet18.ipynb     Untitled0.ipynb\n"," E1285284_CheahWengSoon       main_scheduler2.ipynb   Untitled1.ipynb\n"," LTSM.ipynb\t\t      old\n"," __MACOSX\t\t      UniqueClassCount-main\n","/content/gdrive/MyDrive/Colab Notebooks/__CS5260/assignment6/ViT with ColossalAI\n","args.py  main.ipynb  requirements.txt  run_demo.sh  vit_benchmark.py\n","data.py  README.md   run_benchmark.sh  test_ci.sh   vit_train_demo.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"MMAp0w56wp4X"},"source":["## Install all the requirement."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMjd261mUN8O","outputId":"59e1c031-7c68-49c0-e0ba-cd58d73fee35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting colossalai>=0.1.12 (from -r requirements.txt (line 1))\n","  Downloading colossalai-0.3.6.tar.gz (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.2.1+cu121)\n","Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.25.2)\n","Requirement already satisfied: tqdm>=4.61.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.2)\n","Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.38.2)\n","Collecting datasets (from -r requirements.txt (line 6))\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (24.0)\n","Collecting pre-commit (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading pre_commit-3.7.0-py2.py3-none-any.whl (204 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (8.1.7)\n","Collecting fabric (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contexttimer (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ninja (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.4.2)\n","Collecting einops (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.6.4)\n","Collecting ray (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.99)\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.0.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/731.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]}],"source":["!set -xe\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"i501Zd4kwp4a"},"source":["## Declaration of the hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfoI0P2LLrTY"},"outputs":[],"source":["# path for saving model\n","OUTPUT_PATH=\"./output_model\"\n","\n","# configuration of parallel group sizes, only used when setting PLUGIN to \"hybrid_parallel\"\n","TP_SIZE=2\n","PP_SIZE=2\n","\n","# number of gpus to use\n","GPUNUM=1\n","\n","# batch size per data parallel group\n","BS=16\n","\n","# learning rate\n","LR=\"2e-4\"\n","\n","# number of epoch\n","EPOCH=3\n","\n","# weight decay\n","WEIGHT_DECAY=0.05\n","\n","# ratio of warmup steps\n","WARMUP_RATIO=0.3"]},{"cell_type":"markdown","metadata":{"id":"370Jmvkgwp4b"},"source":["run vit_train_demo.py with Colossal-AI CLI, model google/vit-base-patch16-224 without"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RT_WyMHYwp4b"},"outputs":[],"source":["# model name or path\n","MODEL=\"google/vit-base-patch16-224\"\n","\n","!colossalai run \\\n","    --nproc_per_node {GPUNUM} \\\n","    vit_train_demo.py \\\n","    --model_name_or_path {MODEL} \\\n","    --output_path {OUTPUT_PATH} \\\n","    --batch_size {BS} \\\n","    --tp_size {TP_SIZE} \\\n","    --pp_size {PP_SIZE} \\\n","    --num_epoch {EPOCH} \\\n","    --learning_rate {LR} \\\n","    --weight_decay {WEIGHT_DECAY} \\\n","    --warmup_ratio {WARMUP_RATIO}"]},{"cell_type":"markdown","metadata":{"id":"MBe9-zFewp4b"},"source":["run vit_train_demo.py with Colossal-AI CLI, model google/vit-base-patch16-224 is being used\n","plugins selected are torch_ddp and gemini."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1xls-Mzwp4b"},"outputs":[],"source":["# model name or path\n","MODEL=\"google/vit-base-patch16-224\"\n","# Define the list of plugins\n","PLUGINS = [\"torch_ddp\", \"torch_ddp_fp16\", \"low_level_zero\", \"gemini\", \"hybrid_parallel\"]\n","# PLUGINS = [\"gemini\", \"hybrid_parallel\"]\n","\n","for PLUGIN in PLUGINS:\n","  !colossalai run \\\n","    --nproc_per_node {GPUNUM} \\\n","    vit_train_demo.py \\\n","    --model_name_or_path {MODEL} \\\n","    --output_path {OUTPUT_PATH} \\\n","    --plugin {PLUGIN} \\\n","    --batch_size {BS} \\\n","    --tp_size {TP_SIZE} \\\n","    --pp_size {PP_SIZE} \\\n","    --num_epoch {EPOCH} \\\n","    --learning_rate {LR} \\\n","    --weight_decay {WEIGHT_DECAY} \\\n","    --warmup_ratio {WARMUP_RATIO}"]},{"cell_type":"markdown","metadata":{"id":"UUmg9jWiwp4b"},"source":["replace with model nateraw/vit-base-patch16-224-cifar10 on the best plugin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NMpzPBYmAd-"},"outputs":[],"source":["# MODEL=\"nateraw/vit-base-patch16-224-cifar10\"\n","# PLUGINS = \"gemini\"\n","\n","# !colossalai run \\\n","#   --nproc_per_node {GPUNUM} \\\n","#   vit_train_demo.py \\\n","#   --model_name_or_path {MODEL} \\\n","#   --output_path {OUTPUT_PATH} \\\n","#   --plugin {PLUGIN} \\\n","#   --batch_size {BS} \\\n","#   --tp_size {TP_SIZE} \\\n","#   --pp_size {PP_SIZE} \\\n","#   --num_epoch {EPOCH} \\\n","#   --learning_rate {LR} \\\n","#   --weight_decay {WEIGHT_DECAY} \\\n","#   --warmup_ratio {WARMUP_RATIO}"]},{"cell_type":"markdown","metadata":{"id":"3eJsae3pwp4c"},"source":["benchmark with different plugin used on the best model = xx."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEaCg08VXC-A"},"outputs":[],"source":["# MODEL=\"google/vit-base-patch16-224\"\n","\n","# for PLUGIN in PLUGINS:\n","#     !colossalai run \\\n","#   --nproc_per_node {GPUNUM} \\\n","#   vit_benchmark.py \\\n","#   --model_name_or_path {PLUGIN}{MODEL} \\\n","#   --mem_cap {MEMCAP} \\\n","#   --plugin {PLUGIN} \\\n","#   --batch_size {BS}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ncc7Udu4wp4c"},"outputs":[],"source":["# MODEL=\"nateraw/vit-base-patch16-224-cifar10\"\n","\n","# for PLUGIN in PLUGINS:\n","#     !colossalai run \\\n","#   --nproc_per_node {GPUNUM} \\\n","#   vit_benchmark.py \\\n","#   --model_name_or_path {PLUGIN}{MODEL} \\\n","#   --mem_cap {MEMCAP} \\\n","#   --plugin {PLUGIN} \\\n","#   --batch_size {BS}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}