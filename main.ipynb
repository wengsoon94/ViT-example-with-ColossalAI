{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd \"/content/gdrive/MyDrive/Colab Notebooks\"\n","!ls\n","%cd \"__CS5260/assignment6/ViT with ColossalAI\"\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZxP88UBxxpm","executionInfo":{"status":"ok","timestamp":1712401832879,"user_tz":-480,"elapsed":20330,"user":{"displayName":"Weng Soon Cheah","userId":"01948259457530973069"}},"outputId":"71e6a999-07f0-4b25-fd7d-2638a758df12"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Colab Notebooks\n"," assignmen3_1.ipynb\t      main-1029_1430.ipynb    UniqueClassCount-main-soon5.zip\n","'Copy of ViT_Cifar10.ipynb'   main.ipynb\t     'Untitled0 (1).ipynb'\n"," __CS5260\t\t      main_resnet18.ipynb     Untitled0.ipynb\n"," E1285284_CheahWengSoon       main_scheduler2.ipynb   Untitled1.ipynb\n"," LTSM.ipynb\t\t      old\n"," __MACOSX\t\t      UniqueClassCount-main\n","/content/gdrive/MyDrive/Colab Notebooks/__CS5260/assignment6/ViT with ColossalAI\n","args.py  main.ipynb    __pycache__  requirements.txt  run_demo.sh  vit_benchmark.py\n","data.py  output_model  README.md    run_benchmark.sh  test_ci.sh   vit_train_demo.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"MMAp0w56wp4X"},"source":["## Install all the requirement."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":111176,"status":"ok","timestamp":1712401945596,"user":{"displayName":"Weng Soon Cheah","userId":"01948259457530973069"},"user_tz":-480},"id":"ZMjd261mUN8O","outputId":"978ed60a-756b-4592-fb69-2858cfbbd16e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting colossalai>=0.1.12 (from -r requirements.txt (line 1))\n","  Downloading colossalai-0.3.6.tar.gz (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.2.1+cu121)\n","Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.25.2)\n","Requirement already satisfied: tqdm>=4.61.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.2)\n","Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.38.2)\n","Collecting datasets (from -r requirements.txt (line 6))\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (24.0)\n","Collecting pre-commit (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading pre_commit-3.7.0-py2.py3-none-any.whl (204 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (13.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (8.1.7)\n","Collecting fabric (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contexttimer (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ninja (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.4.2)\n","Collecting einops (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.6.4)\n","Collecting ray (from colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.99)\n","Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.0.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->-r requirements.txt (line 2))\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (0.20.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (0.15.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 6))\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (2.0.3)\n","Collecting xxhash (from datasets->-r requirements.txt (line 6))\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 6))\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.9.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2024.2.2)\n","Collecting invoke>=2.0 (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting decorator>=5 (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting deprecated>=1.2 (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.12.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->-r requirements.txt (line 2)) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2024.1)\n","Collecting cfgv>=2.0.0 (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n","Collecting identify>=1.0.0 (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading identify-2.5.35-py2.py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n","Collecting virtualenv>=20.10.0 (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading virtualenv-20.25.1-py3-none-any.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.0.8)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (67.7.2)\n","Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (42.0.5)\n","Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 6)) (1.16.0)\n","Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1))\n","  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.18.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.22)\n","Building wheels for collected packages: colossalai, contexttimer\n","  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colossalai: filename=colossalai-0.3.6-py3-none-any.whl size=1389734 sha256=2a719cdc70d05bd514abce3f647d0b071ebe211331a2d77e8ccb549a0fae1d66\n","  Stored in directory: /root/.cache/pip/wheels/4c/52/8e/8ff9fb0a6ec328844d9344eb1e1adc29f3d05886adb2a3551a\n","  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=78d0c941fd6159517099438fc78c5149a7919fbc63c8cf4e7bb406d60067d692\n","  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n","Successfully built colossalai contexttimer\n","Installing collected packages: ninja, distlib, contexttimer, xxhash, virtualenv, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nodeenv, invoke, identify, einops, dill, deprecated, decorator, cfgv, bcrypt, pynacl, pre-commit, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, paramiko, nvidia-cusolver-cu12, ray, fabric, datasets, colossalai\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 4.4.2\n","    Uninstalling decorator-4.4.2:\n","      Successfully uninstalled decorator-4.4.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bcrypt-4.1.2 cfgv-3.4.0 colossalai-0.3.6 contexttimer-0.3.3 datasets-2.18.0 decorator-5.1.1 deprecated-1.2.14 dill-0.3.8 distlib-0.3.8 einops-0.7.0 fabric-3.2.2 identify-2.5.35 invoke-2.2.0 multiprocess-0.70.16 ninja-1.11.1.1 nodeenv-1.8.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 paramiko-3.4.0 pre-commit-3.7.0 pynacl-1.5.0 ray-2.10.0 virtualenv-20.25.1 xxhash-3.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["decorator"]},"id":"f23bc45bf7d24d6cb784de800a6bfd70"}},"metadata":{}}],"source":["!set -xe\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"i501Zd4kwp4a"},"source":["## Declaration of the hyperparameter"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ROVPe952fqi","executionInfo":{"status":"ok","timestamp":1712401376811,"user_tz":-480,"elapsed":1088507,"user":{"displayName":"Weng Soon Cheah","userId":"01948259457530973069"}},"outputId":"78b0089a-c470-4743-848d-5c684b1c6642"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:44:57] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:45:00] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.0921468734741211 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11187267303466797 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:45:01] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:06<00:00,  2.98it/s]\n","[04/06/24 10:45:08] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: torch_ddp, throughput: 23.8659, maximum memory usage per    \n","                             gpu: 1.74 GB.                                                          \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:45:19] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:45:21] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.1334836483001709 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.16259098052978516 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:45:22] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n","[04/06/24 10:45:48] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             32, plugin: torch_ddp, throughput: 25.1587, maximum memory usage per   \n","                             gpu: 2.11 GB.                                                          \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:45:58] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:46:00] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.1367051601409912 seconds\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Time taken to compile fused_optim_cuda op: 0.16158580780029297 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:46:01] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:54<00:00,  2.71s/it]\n","[04/06/24 10:46:56] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             64, plugin: torch_ddp, throughput: 23.6239, maximum memory usage per   \n","                             gpu: 2.68 GB.                                                          \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:47:07] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:47:10] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09125447273254395 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10834288597106934 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:47:11] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [01:44<00:00,  5.23s/it]\n","[04/06/24 10:48:56] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             128, plugin: torch_ddp, throughput: 24.4560, maximum memory usage per  \n","                             gpu: 4.02 GB.                                                          \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:49:08] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:49:09] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09169650077819824 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10866689682006836 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:49:11] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [03:34<00:00, 10.73s/it]\n","[04/06/24 10:52:45] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             256, plugin: torch_ddp, throughput: 23.8663, maximum memory usage per  \n","                             gpu: 6.69 GB.                                                          \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:52:57] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:52:58] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09848308563232422 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11029672622680664 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:52:59] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:02<00:00,  6.83it/s]\n","[04/06/24 10:53:02] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: torch_ddp_fp16, throughput: 54.6009, maximum memory usage   \n","                             per gpu: 1.73 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:53:16] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:53:18] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09665846824645996 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11290764808654785 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:53:19] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:08<00:00,  2.32it/s]\n","[04/06/24 10:53:28] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             32, plugin: torch_ddp_fp16, throughput: 74.0758, maximum memory usage  \n","                             per gpu: 2.03 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:53:40] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:53:42] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09491586685180664 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1087188720703125 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:53:43] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n","[04/06/24 10:54:00] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             64, plugin: torch_ddp_fp16, throughput: 76.9347, maximum memory usage  \n","                             per gpu: 2.50 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:54:14] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:54:16] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09298229217529297 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10906004905700684 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:54:17] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:33<00:00,  1.69s/it]\n","[04/06/24 10:54:51] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             128, plugin: torch_ddp_fp16, throughput: 75.6864, maximum memory usage \n","                             per gpu: 3.64 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:55:05] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:55:06] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.0935673713684082 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1095120906829834 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:55:07] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [01:07<00:00,  3.37s/it]\n","[04/06/24 10:56:15] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             256, plugin: torch_ddp_fp16, throughput: 75.8536, maximum memory usage \n","                             per gpu: 5.91 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:56:28] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","[04/06/24 10:56:29] INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:56:30] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09331226348876953 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10729646682739258 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:56:31] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:04<00:00,  4.11it/s]\n","[04/06/24 10:56:36] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: low_level_zero, throughput: 32.8720, maximum memory usage   \n","                             per gpu: 1.66 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:56:49] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:56:50] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.1055757999420166 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10728764533996582 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:56:51] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:08<00:00,  2.28it/s]\n","[04/06/24 10:57:00] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             32, plugin: low_level_zero, throughput: 72.9648, maximum memory usage  \n","                             per gpu: 1.67 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:57:12] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:57:14] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09252262115478516 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.11229777336120605 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:57:15] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n","[04/06/24 10:57:31] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             64, plugin: low_level_zero, throughput: 82.9333, maximum memory usage  \n","                             per gpu: 1.88 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:57:41] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:57:43] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09109759330749512 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10730886459350586 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:57:44] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:29<00:00,  1.48s/it]\n","[04/06/24 10:58:14] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             128, plugin: low_level_zero, throughput: 86.5683, maximum memory usage \n","                             per gpu: 2.59 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:58:26] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:58:27] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09222102165222168 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10967040061950684 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","[04/06/24 10:58:28] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  numel += p.storage().size()\n","Training Step: 100%|██████████| 20/20 [00:57<00:00,  2.87s/it]\n","[04/06/24 10:59:26] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             256, plugin: low_level_zero, throughput: 89.2643, maximum memory usage \n","                             per gpu: 4.00 GB.                                                      \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:59:35] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 10:59:37] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.15205168724060059 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.16410136222839355 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/06/24 10:59:40] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n","[04/06/24 10:59:47] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             8, plugin: gemini, throughput: 22.4066, maximum memory usage per gpu:  \n","                             663.17 MB.                                                             \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 10:59:59] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 11:00:02] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.13025975227355957 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.16254448890686035 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/06/24 11:00:05] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n","[04/06/24 11:00:16] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             32, plugin: gemini, throughput: 60.2019, maximum memory usage per gpu: \n","                             663.17 MB.                                                             \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 11:00:28] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 11:00:31] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.10187244415283203 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1088864803314209 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/06/24 11:00:33] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n","[04/06/24 11:00:50] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             64, plugin: gemini, throughput: 75.0164, maximum memory usage per gpu: \n","                             928.37 MB.                                                             \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 11:01:03] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 11:01:04] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09945464134216309 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.10850977897644043 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/06/24 11:01:07] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:31<00:00,  1.56s/it]\n","[04/06/24 11:01:38] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             128, plugin: gemini, throughput: 81.8921, maximum memory usage per gpu:\n","                             1.61 GB.                                                               \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 11:01:53] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","[04/06/24 11:01:54] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:68 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:94 \n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 0.09438037872314453 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 0.1087486743927002 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/06/24 11:01:57] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:108\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Start testing                          \n","Training Step: 100%|██████████| 20/20 [00:59<00:00,  2.97s/it]\n","[04/06/24 11:02:56] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with ColossalAI/vit_benchmark.py:140\n","                             main                                                                   \n","                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n","                             256, plugin: gemini, throughput: 86.2138, maximum memory usage per gpu:\n","                             3.02 GB.                                                               \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n"]}],"source":["MODEL_PATH=\"google/vit-base-patch16-224\"\n","MEMCAP=0\n","GPUNUM=1\n","BS = [8,32,64,128,256]\n","\n","PLUGINS = [\"torch_ddp\", \"torch_ddp_fp16\", \"low_level_zero\", \"gemini\"]\n","\n","for PLUGIN in PLUGINS:\n","  for _BS in BS:\n","    !colossalai run \\\n","      --nproc_per_node {GPUNUM} \\\n","      vit_benchmark.py \\\n","      --model_name_or_path {MODEL_PATH} \\\n","      --mem_cap {MEMCAP} \\\n","      --plugin {PLUGIN} \\\n","      --batch_size {_BS}"]},{"cell_type":"markdown","metadata":{"id":"370Jmvkgwp4b"},"source":["run vit_train_demo.py with Colossal-AI CLI, model google/vit-base-patch16-224 without"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RT_WyMHYwp4b","executionInfo":{"status":"ok","timestamp":1712402640735,"user_tz":-480,"elapsed":540115,"user":{"displayName":"Weng Soon Cheah","userId":"01948259457530973069"}},"outputId":"07e18b86-58bb-4a1a-e052-ff060c9640a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-06 11:15:10.420932: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-06 11:15:10.420979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-06 11:15:10.537521: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-06 11:15:12.484396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n","  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n","/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n","  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n","[04/06/24 11:15:15] INFO     colossalai - colossalai - INFO:                                        \n","                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n","                             launch                                                                 \n","                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n","                             world size: 1                                                          \n","preprocessor_config.json: 100%|██████████| 160/160 [00:00<00:00, 766kB/s]\n","Downloading readme: 100%|██████████| 4.95k/4.95k [00:00<00:00, 18.2MB/s]\n","Downloading data: 100%|██████████| 144M/144M [00:10<00:00, 13.6MB/s]\n","Downloading data: 100%|██████████| 18.5M/18.5M [00:01<00:00, 13.0MB/s]\n","Downloading data: 100%|██████████| 17.7M/17.7M [00:01<00:00, 15.4MB/s]\n","Generating train split: 100%|██████████| 1034/1034 [00:00<00:00, 1645.43 examples/s]\n","Generating validation split: 100%|██████████| 133/133 [00:00<00:00, 2247.36 examples/s]\n","Generating test split: 100%|██████████| 128/128 [00:00<00:00, 2302.04 examples/s]\n","config.json: 100%|██████████| 69.7k/69.7k [00:00<00:00, 1.07MB/s]\n","model.safetensors: 100%|██████████| 346M/346M [00:01<00:00, 206MB/s]\n","Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[04/06/24 11:15:56] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with                                \n","                             ColossalAI/vit_train_demo.py:171 main                                  \n","                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n","                             google/vit-base-patch16-224                                            \n","                    INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with                                \n","                             ColossalAI/vit_train_demo.py:199 main                                  \n","                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n","[extension] Time taken to compile cpu_adam_x86 op: 37.98287558555603 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n","  warnings.warn(\n","[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n","[extension] Time taken to compile fused_optim_cuda op: 241.9109365940094 seconds\n","/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n","/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return tensor.storage().size() == 0\n","[04/06/24 11:20:38] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with                                \n","                             ColossalAI/vit_train_demo.py:230 main                                  \n","                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n","Epoch [1]: 100%|██████████| 32/32 [00:17<00:00,  1.79it/s, loss=0.292]\n","Evaluation result for epoch 1:                 average_loss=0.2546,                 accuracy=0.9375.\n","Epoch [2]: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s, loss=0.0128]\n","Evaluation result for epoch 2:                 average_loss=0.0505,                 accuracy=0.9844.\n","Epoch [3]:  97%|█████████▋| 31/32 [00:15<00:00,  1.97it/s, loss=0.00268]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","Epoch [3]: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s, loss=0.00602]\n","Evaluation result for epoch 3:                 average_loss=0.0135,                 accuracy=1.0000.\n","Epoch [4]: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s, loss=0.00302]\n","Evaluation result for epoch 4:                 average_loss=0.0248,                 accuracy=0.9844.\n","Epoch [5]: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s, loss=0.000712]\n","Evaluation result for epoch 5:                 average_loss=0.0105,                 accuracy=1.0000.\n","Epoch [6]: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s, loss=0.00059]\n","Epoch [7]:   0%|          | 0/32 [00:00<?, ?it/s]Evaluation result for epoch 6:                 average_loss=0.0098,                 accuracy=1.0000.\n","Epoch [7]: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s, loss=0.000513]\n","Evaluation result for epoch 7:                 average_loss=0.0086,                 accuracy=1.0000.\n","Epoch [8]: 100%|██████████| 32/32 [00:17<00:00,  1.88it/s, loss=0.000475]\n","Evaluation result for epoch 8:                 average_loss=0.0082,                 accuracy=1.0000.\n","Epoch [9]: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s, loss=0.00046]\n","Evaluation result for epoch 9:                 average_loss=0.0080,                 accuracy=1.0000.\n","Epoch [10]: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s, loss=0.000458]\n","Evaluation result for epoch 10:                 average_loss=0.0079,                 accuracy=1.0000.\n","[04/06/24 11:23:30] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with                                \n","                             ColossalAI/vit_train_demo.py:234 main                                  \n","                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n","[04/06/24 11:23:54] INFO     colossalai - colossalai - INFO: /content/gdrive/MyDrive/Colab          \n","                             Notebooks/__CS5260/assignment6/ViT with                                \n","                             ColossalAI/vit_train_demo.py:238 main                                  \n","                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n","                             ./output_model                                                         \n","\n","====== Training on All Nodes =====\n","127.0.0.1: success\n","\n","====== Stopping All Nodes =====\n","127.0.0.1: finish\n"]}],"source":["# path for saving model\n","OUTPUT_PATH=\"./output_model\"\n","\n","# configuration of parallel group sizes, only used when setting PLUGIN to \"hybrid_parallel\"\n","TP_SIZE=2\n","PP_SIZE=2\n","\n","# number of gpus to use\n","GPUNUM=1\n","\n","# batch size per data parallel group\n","BS=32\n","\n","# learning rate\n","LR=\"1e-4\"\n","\n","# number of epoch\n","EPOCH=10\n","\n","# weight decay\n","WEIGHT_DECAY=0.05\n","\n","# ratio of warmup steps\n","WARMUP_RATIO=0.3\n","\n","# model name or path\n","MODEL=\"google/vit-base-patch16-224\"\n","PLUGIN = \"gemini\"\n","\n","!colossalai run \\\n","  --nproc_per_node {GPUNUM} \\\n","  vit_train_demo.py \\\n","  --model_name_or_path {MODEL} \\\n","  --output_path {OUTPUT_PATH} \\\n","  --plugin {PLUGIN} \\\n","  --batch_size {BS} \\\n","  --tp_size {TP_SIZE} \\\n","  --pp_size {PP_SIZE} \\\n","  --num_epoch {EPOCH} \\\n","  --learning_rate {LR} \\\n","  --weight_decay {WEIGHT_DECAY} \\\n","  --warmup_ratio {WARMUP_RATIO}"]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"CSPXWpYAAmUs"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}